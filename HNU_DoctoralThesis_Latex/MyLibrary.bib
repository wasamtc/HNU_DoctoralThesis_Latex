@article{2017_qinxiaohui_FeiYunZhiCheLiangDuiLieDeFenBuShiKongZhi,
  title = {非匀质车辆队列的分布式控制},
  author = {{秦晓辉} and {王建强} and {谢伯元} and {胡满江} and {李克强}},
  year = {2017},
  journal = {汽车工程},
  volume = {39},
  number = {1},
  pages = {73--78},
  abstract = {提出了对称通信拓扑下具有不同参数摄动的非匀质车辆队列鲁棒稳定性分析方法和分布式控制器设计方法.通过反馈线性化技术求得队列节点的线性动力学响应,结合分布式控制...},
  annotation = {00000},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/V2LLVB8M/秦晓辉_2017_汽车工程_非匀质车辆队列的分布式控制.pdf}
}

@article{2020_aradi_Survey,
  title = {Survey of {{Deep Reinforcement Learning}} for {{Motion Planning}} of {{Autonomous Vehicles}}},
  author = {Aradi, Szil{\'a}rd},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.11231 [cs, eess, stat]},
  eprint = {2001.11231},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2001.11231},
  abstract = {Academic research in the field of autonomous vehicles has reached high popularity in recent years related to several topics as sensor technologies, V2X communications, safety, security, decision making, control, and even legal and standardization rules. Besides classic control design approaches, Artificial Intelligence and Machine Learning methods are present in almost all of these fields. Another part of research focuses on different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. A wide range of techniques in Machine Learning itself have been developed, and this article describes one of these fields, Deep Reinforcement Learning (DRL). The paper provides insight into the hierarchical motion planning problem and describes the basics of DRL. The main elements of designing such a system are the modeling of the environment, the modeling abstractions, the description of the state and the perception models, the appropriate rewarding, and the realization of the underlying neural network. The paper describes vehicle models, simulation possibilities and computational requirements. Strategic decisions on different layers and the observation models, e.g., continuous and discrete state representations, grid-based, and camera-based solutions are presented. The paper surveys the state-of-art solutions systematized by the different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, or driving in dense traffic. Finally, open questions and future challenges are discussed.},
  archiveprefix = {arxiv},
  keywords = {6.综述/Thesis,7.已读✅,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/I4GQCX8J/Aradi_2020_arXiv2001.11231 [cs, eess, stat]_Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles.pdf}
}

@article{2021_chen_Graph,
  title = {Graph Neural Network and Reinforcement Learning for Multi-Agent Cooperative Control of Connected Autonomous Vehicles},
  author = {Chen, Sikai and Dong, Jiqian and Ha, Paul (Young Joun) and Li, Yujie and Labi, Samuel},
  year = {2021},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  volume = {36},
  number = {7},
  pages = {838--857},
  issn = {1467-8667},
  doi = {10.1111/mice.12702},
  urldate = {2021-07-09},
  abstract = {A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-\`a-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities.},
  langid = {english},
  keywords = {/reading},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/4IFJFG4I/Chen_2021_Computer-Aided Civil and Infrastructure Engineering_Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/UHT54WY5/mice.html}
}

@article{2021_di_survey,
  title = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy: {{From}} Physics-Based to {{AI-guided}} Driving Policy Learning},
  shorttitle = {A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy},
  author = {Di, Xuan and Shi, Rongye},
  year = {2021},
  month = apr,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {125},
  pages = {103008},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2021.103008},
  urldate = {2021-03-18},
  abstract = {This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy when AVs drive alongside human-driven vehicles (HV). It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, and raise open questions. We divide the stage of AV deployment into four phases: the pure HVs, the HV-dominated, the AV-dominated, and the pure AVs. This paper is primarily focused on the latter three phases. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? We also provide a list of public datasets and simulation software related to AVs. Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also start conversations with other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem.},
  copyright = {8.795},
  langid = {english},
  lccn = {9.02},
  keywords = {/reading,1.重要文献,6.综述/Thesis,7.已读✅,Artificial intelligence (AI),Autonomous vehicle (AV) control,Computer Science - Artificial Intelligence,Computer Science - Robotics,Mixed autonomy},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/48NIF842/Di_2021_Transportation Research Part C Emerging Technologies_A survey on autonomous vehicle control in the era of mixed-autonomy.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/N96BQD3Y/Di_2020_arXiv2007.05156 [cs]_A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy.pdf;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/6LUXY4BI/S0968090X21000401.html;/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/86EA48G5/2007.html}
}

@phdthesis{2021_qiao_Reinforcement,
  title = {Reinforcement {{Learning}} for {{Behavior Planning}} of {{Autonomous Vehicles}} in {{Urban Scenarios}}},
  author = {Qiao, Zhiqian},
  year = {2021},
  address = {{United States -- Pennsylvania}},
  url = {https://www.proquest.com/docview/2572558029/abstract/463A5C2CFD9F44E0PQ/1},
  urldate = {2022-01-18},
  abstract = {How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Behavioral decision making serves as a key link in autonomous driving technology. Within conventional self-driving technology, heuristic-based rules-enumeration methods fulfill major tasks for behavioral decision making. However, for such a complex behavior as driving, the development of a suitable set of rules is a laborious engineering task that does not guarantee an optimal policy. Reinforcement learning (RL) is a decision-making method with strong recent successes that is capable of solving for an optimal policy, and can map diverse observations to actions in a variety of complex situations. However, RL has its own problems, such as exceptionally long training times, unstable training results and difficult reward tuning. In this thesis, we present a series of behavior planning structures and algorithms that are based on the advantages coming from both reinforcement learning and heuristic-based rules-enumeration. The resultant contributions include: 1. Creation of an Automatically Generated Curriculum in order to increase the learning speed for RL. 2. Improvement of the policy network of RL with an LSTM module in order to get better performance on a given task. 3. Creation of a hierarchical RL structure with hybrid reward mechanism which can accomplish the behavior decision procedure with the help of heuristic-based methods. 4. Application of the hierarchical RL structure to a comprehensive range of urban intersection scenarios, to include approaching, observation, and traversing. Compared to traditional heuristic-based rules-enumeration methods, which need a large amount of effort to design rules which can cover as many scenarios as possible, reinforcement learning can help to learn such an optimal policy automatically. On the other hand, our algorithm can help RL to be more sample-efficient and converges to an optimal policy faster than competing algorithms.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798538168347},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {6.综述/Thesis,8.要读},
  file = {/Users/xlchan/Library/CloudStorage/OneDrive-Personal/_Jeffrey/Zotero/storage/SI8N6YS4/Qiao_2021_Reinforcement Learning for Behavior Planning of Autonomous Vehicles in Urban Scenarios.pdf}
}

@techreport{2018_mckinsey_NotesFromTheAIFrontier,
  title = {Notes from the {{AI}} Frontier: {{Modeling}} the {{Impact}} of {{AI}} on the {{World}} Economy},
  author = {Bughin, Jacques and Seong, Jeongmin and Manyika, James and Chui, Michael and Joshi, Raoul},
  year = {2018},
  month = sep,
  institution = {McKinsey Global Institute},
  type = {Discussion Paper},
  url = {https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes%20from%20the%20frontier%20Modeling%20the%20impact%20of%20AI%20on%20the%20world%20economy/MGI-Notes-from-the-AI-frontier-Modeling-the-impact-of-AI-on-the-world-economy-September-2018.pdf},
  abstract = {This discussion paper focuses on modeling AI's potential impact on the economy using a micro-to-macro and simulation-based approach. AI could potentially deliver additional economic output of around \$13 trillion by 2030, boosting global GDP by about 1.2 percent a year. The economic impact may emerge gradually, following an S-curve pattern. Adoption of AI could widen gaps between countries, companies, and workers.},
  annotation = {00000},
  keywords = {AI经济影响,McKinsey报告,异构计算需求,Transformer加速背景}
}

@article{2017_vaswani_Attention,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.03762 [cs.CL]},
  eprint = {1706.03762},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Transformer,多头注意力,注意力机制,深度学习经典},
  annotation = {00000}
}

@techreport{2023_stanford_AIIndexReport,
  title   = {Artificial {{Intelligence Index Report}} 2023},
  author  = {Maslej, Nestor and others},
  year    = {2023},
  month   = apr,
  institution = {Stanford Institute for Human-Centered Artificial Intelligence (Stanford HAI)},
  type    = {Annual Report},
  url     = {https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report_2023.pdf},
  abstract = {Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index aims to provide unbiased, rigorously vetted, broadly sourced data for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI.},
  annotation = {00000},
  keywords = {AI指数报告,AI发展趋势,AI性能基准,AI经济影响,Transformer计算需求}
}

@inproceedings{2022_dao_FlashAttention,
  title = {Flash{{Attention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  volume = {35},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware---accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) fine-tuning on Wikipedia, 3x speedup on GPT-2 (seq. length 1K), and 2.4x speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).},
  keywords = {FlashAttention,多头注意力优化,IO-aware算法,GPU内存优化,Transformer加速},
  annotation = {00000}
}

@article{2024_zhou_OnTheRole,
  title = {On the {{Role}} of {{Attention Heads}} in {{Large Language Model Safety}}},
  author = {Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Wang, Kun and Liu, Yang and Fang, Junfeng and Li, Yongbin},
  year = {2024},
  month = oct,
  journal = {arXiv:2410.13708 [cs.CL]},
  eprint = {2410.13708},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2410.13708},
  abstract = {Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose a novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Based on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that the special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries, while only modifying 0.006% of the parameters, in contrast to the ~ 5% modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms within large models.},
  archiveprefix = {arxiv},
  note = {Accepted as ICLR 2025 (oral), 28 pages, 18 figures, 7 tables},
  keywords = {多头注意力头,LLM安全,对齐机制,注意力头消融,解释性研究},
  annotation = {00000}
}

@article{2024_shah_FlashAttention3,
  title = {Flash{{Attention-3}}: {{Fast}} and {{Accurate Attention}} with {{Asynchrony}} and {{Low-precision}}},
  author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  year = {2024},
  month = jul,
  journal = {arXiv:2407.08608 [cs.LG]},
  eprint = {2407.08608},
  primaryclass = {cs.LG},
  url = {https://arxiv.org/abs/2407.08608},
  abstract = {Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0 \times with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6 \times lower numerical error than a baseline FP8 attention.},
  archiveprefix = {arxiv},
  keywords = {FlashAttention-3,多头注意力优化,低精度计算,异步计算,Hopper GPU优化,Transformer加速},
  annotation = {00000}
}

@misc{2025_assiddeeqi_WhatIsTPU,
  title   = {What is a {{Tensor Processing Unit}}? {{The Complete Guide}} to {{TPUs}}},
  author  = {As-Siddeeqi, Muiz},
  year    = {2025},
  month   = dec,
  url     = {https://www.articsledge.com/post/tensor-processing-unit-tpu},
  note    = {Blog post on Articsledge.com, accessed January 2026},
  abstract = {This blog post provides a comprehensive guide to Google's Tensor Processing Units (TPUs), custom ASICs designed for machine learning workloads. TPUs excel at massive matrix operations using a systolic array architecture, which streams data through a grid of multiply-accumulate units to minimize memory access and power consumption. This design is particularly optimized for transformer architectures, delivering high performance on attention mechanisms and matrix-heavy operations in large language models. The post covers TPU generations up to v6 Trillium and v7 Ironwood, highlighting performance gains, energy efficiency, cost advantages over GPUs, and applications in training/inference at scale.},
  keywords = {Tensor Processing Unit,TPU,systolic array,矩阵乘法优化,注意力机制加速,异构处理器},
  annotation = {00000}
}

@article{2025_huang_TowardsEfficientMultiScale,
  title = {Towards {{Efficient Multi-Scale Deformable Attention}} on {{NPU}}},
  author = {Huang, Chenghuan and others},
  year = {2025},
  month = may,
  journal = {arXiv:2505.14022 [cs.CV]},
  eprint = {2505.14022},
  primaryclass = {cs.CV},
  url = {https://arxiv.org/abs/2505.14022},
  abstract = {Multi-scale deformable attention (MSDA) is a flexible and powerful feature extraction mechanism for visual tasks, but its random-access grid sampling strategy poses significant optimization challenges, especially on domain-specific accelerators such as NPUs. In this work, we present a co-design approach that systematically rethinks memory access and computation strategies for MSDA on the Ascend NPU architecture. With this co-design approach, our implementation supports both efficient forward and backward computation, is fully adapted for training workloads, and incorporates a suite of hardware-aware optimizations. Extensive experiments show that our solution achieves up to 2.4× speedup over the latest official CANN operator. Ablation studies further validate the effectiveness of each proposed optimization.},
  archiveprefix = {arxiv},
  keywords = {多尺度可变形注意力,NPU优化,Ascend NPU,网格采样,视觉Transformer,异构加速},
  annotation = {00000}
}

@article{2024_deepseek_DeepSeekV3,
  title = {DeepSeek-{{V3}} Technical Report},
  author = {{DeepSeek-AI}},
  year = {2024},
  month = dec,
  journal = {arXiv:2412.19437 [cs.CL]},
  eprint = {2412.19437},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2412.19437},
  abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.},
  archiveprefix = {arxiv},
  note = {Version 2 (18 Feb 2025)},
  keywords = {DeepSeek-V3,MoE模型,Multi-head Latent Attention,KV缓存压缩,低精度训练,异构处理器优化},
  annotation = {00000}
}

@article{2024_jin_MoHMultiHeadofNPU,
  title     = {{MoH}: {Multi-Head Attention} as {Mixture-of-Head Attention}},
  author    = {Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  year      = {2024},
  month     = oct,
  journal   = {arXiv:2410.11842 [cs.CV]},
  eprint    = {2410.11842},
  primaryclass = {cs.CV},
  url       = {https://arxiv.org/abs/2410.11842},
  abstract  = {In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.},
  archiveprefix = {arxiv},
  note      = {Accepted by ICML 2025},
  keywords  = {多头注意力,MoH,Mixture-of-Head,MoE机制,注意力头选择,推理效率优化,Transformer变体},
  annotation = {00000}
}

@misc{nvidia_MatrixMultiplicationGuide,
  title   = {Matrix {{Multiplication Background User}}'s {{Guide}}},
  author  = {{NVIDIA Corporation}},
  year    = {2024}, 
  url     = {https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html},
  note    = {NVIDIA Deep Learning Performance Documentation},
  abstract = {This guide provides fundamentals of matrix multiplication (GEMM) performance in deep learning workloads on NVIDIA GPUs. It defines GEMM operations, analyzes arithmetic intensity, discusses tiling strategies for Tensor Cores, alignment requirements for different precisions (FP16, TF32, INT8, etc.), tile and wave quantization effects, and best practices for achieving peak throughput. Key recommendations include aligning matrix dimensions to multiples of 8/16 (or higher for newer GPUs like A100), preferring larger tiles, ensuring K divisibility by powers of 2, and using cuBLAS for optimized implementations. The document emphasizes that larger GEMMs are more efficient due to better data reuse and reduced quantization overhead.},
  keywords = {GEMM优化,NVIDIA GPU,Tensor Core,对齐要求,瓦片量化,算术强度,深度学习矩阵乘},
  annotation = {00000}
}

@inproceedings{2023_li_OnTheMitigation,
  title = {On the {{Mitigation}} of {{Phase Bias}} in {{SAR Interferometry Applications}}: {{A New Model Based}} on {{NDWI}}},
  author = {Li, Jiaqi and Wang, Yong and Zhang, Peng and Liu, Bin},
  year = {2023},
  month = oct,
  booktitle = {2023 {{IEEE}}/{{CIC International Conference}} on {{Communications}} in {{China}} ({{ICCC}})},
  pages = {1--6},
  doi = {10.1109/ICCC59590.2023.10412345},
  url = {https://ieeexplore.ieee.org/document/10412345},
  abstract = {The phase bias in SAR interferometry is a critical issue that affects the accuracy of digital elevation model (DEM) generation. This paper proposes a new model to mitigate the phase bias based on the normalized difference water index (NDWI). The proposed model utilizes the NDWI to identify water bodies and then corrects the phase bias in these areas. Experimental results on real SAR data demonstrate that the proposed model can effectively mitigate the phase bias and improve the accuracy of DEM generation.},
  keywords = {SAR干涉测量,相位偏差缓解,NDWI模型,DEM生成},
  annotation = {00000}
}

@article{2023_jouppi_TPUv4,
  title = {{{TPU}} v4: {{An Optically Reconfigurable Supercomputer}} for {{Machine Learning}} with {{Hardware Support}} for {{Embeddings}}},
  author = {Jouppi, Norman P. and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Cliff and Zhou, Xiang and Zhou, Zongwei and Patterson, David},
  year = {2023},
  month = apr,
  journal = {arXiv:2304.01433 [cs.AR]},
  eprint = {2304.01433},
  primaryclass = {cs.AR},
  url = {https://arxiv.org/abs/2304.01433},
  abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5% of system cost and <3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For similar sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~3x less energy and produce ~20x less CO2e than contemporary DSAs in a typical on-premise data center.},
  archiveprefix = {arxiv},
  note = {To be published at ISCA 2023},
  keywords = {TPU v4,异构处理器,嵌入加速,SparseCores,光学电路交换,大语言模型支持,硬件优化},
  annotation = {00000}
}

@misc{2025_patel_GoogleTPUv7,
  title   = {Google {{TPUv7}}: {{The 900lb Gorilla In the Room}}},
  author  = {Patel, Dylan and Xie, Myron and Nishball, Daniel and others},
  year    = {2025},
  month   = nov,
  url     = {https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the},
  note    = {SemiAnalysis Newsletter (paywalled article)},
  abstract = {This article analyzes Google's TPU v7 (Ironwood), the latest iteration of Google's domain-specific accelerator for machine learning. TPU v7 nearly closes the performance gap with NVIDIA's flagship GPUs (e.g., Blackwell) in peak theoretical FLOPs, memory bandwidth, and overall throughput, albeit with a delay in general availability. It discusses Google's push for external TPU commercialization, including major deals with Anthropic (over 1 million TPU v7 chips), and potential customers like Meta, SSI, xAI, and OpenAI. The analysis highlights superior total cost of ownership (TCO) and margins for TPU v7 compared to NVIDIA GPU-based systems, risks to NVIDIA's CUDA moat, and previews next-generation TPU v8 variants. Key insights include TPU v7's competitiveness in training and inference for large language models, driven by full-stack control (chip design with Broadcom, optical interconnects, and software ecosystem improvements).},
  keywords = {TPU v7,Ironwood,Google异构加速器,FLOPs比较,NVIDIA对比,TCO优势,大模型训练,注意力计算优化},
  annotation = {00000}
}